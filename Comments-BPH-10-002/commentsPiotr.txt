Hi Piotr,

thanks for your careful reading of the paper. Replies follow.

> *** Physics Comments:
> Line 56-57: while interesting piece of information, we don't see how
> relevant it is that there were 1.6 collisions per crossing.

It is, in the sense that if this number was >> 1 the primary vertex 
misassignment based on the closest vertex in z (Section 6) could have been
a serious effect. In this case the 3rd Gaussian fixed from simulation does
the job.

> Line 58: "good quality data" is not immediately clear, would suggest to
> replace with something like "Tracker, the Muon and the luminosity
> measurement detectors were fully operational".

OK.

> Line 60: what is the motivation for using only a double-muon trigger?

Because the trigger efficiency determined from tag-and-probe and using 
a mixture of triggers with different prescale factors makes this method fail.
In the ICHEP analysis, a single muon trigger was used in some bins, but in 
this larger dataset the lowest unprescaled threshold is 5 GeV at HLT, which is
significantly less efficient than the double muon.

> Line 68-74: It is not clear for a non-expert if the MC generators listed
> here have all the improvements listed on lines 15-19. It would be nice to
> clarify.

It's not really relevant, due to the very limited use of MC in the analysis.
In the acceptance determination, possible pT spectrum variations due to the
presence/absence of these improvements have been taken as a systematic
uncertainty.

> Line 84: define "central" and "forward" in terms of (pseudo)rapidity.

OK.

> Line 91: would recommend changing "cuts are applied" to e.g.: "To
> reduce backgrounds from fake muons ..., muon tracks are required to pass
> the following requirements".

OK.

> Line 92: we guess it is at least two tracks in the pixel layer, not
> exactly two?

Yes, this is a typo.

> Line 99-103 and Table 1: The purpose of this part and some details are a
> little unclear. Is the correction derived using the J/psi peak also (and
> does this not cause a bias)? Is this correction then applied to the J/psi
> studies? The correction is very small and almost compatible with zero
> " does it have any measurable effect on the results?

The last answer says it all. No, it has not a measurable effect on the 
result, when compared to other systematics (see Table 2 in the NEW draft,
a typo was there in the old one, as you also noticed in your comments
below).

> Line 111: Why do you choose the largest pt muon pair? Does this affect the
> kinematics of the non-prompt J/Psi?

See answers to FNAL. The purity is the real discriminating variable when 
two pairs are there, this choice is made only when two pairs with the SAME
purity are found, i.e. in much less than 7% of the events, making the effect
negligible.

> Page 3-4: what is the reason for suddenly switching from pseudo-rapidity
> to rapidity?

There is no switch. For J/psi we use always y, for single muons always eta
(which equals y, since the muons are almost massless).

> Line 144-147: this section needs a little clarification: what is the
> polarization "predicted by EvtGen"? How were the events with
> "polarization measured at the BaBar experiment" generated?

By simply reweighting the events produced by EvtGen. This section will be
expanded, as many reviewers commented on this part.

> Line
> 150-181: and Table 3, it would be nice to label the bullets in a way
> corresponding to the table column labeling. 

OK.

> It would be also nice to have
> the order of magnitude of the different systematic effects mentioned in
> the text, pointing out which are the largest ones.
> What is the meaning of the big fluctuations, eg. of the "Fit
> function" systematic? 

This part has been changed in the new draft.

> What does the column "pT
> calibration" refer to? If the calibration mentioned in Table 1, why
> is the systematic so large?

Because of a typo, now corrected. See also answer above on momentum scale.

> Line 161: What is the motivation for the choice of 20%? Maybe a reference?



> Line 199-204: please quote the order of magnitude of the tag selection bias.

It is quantified by rho (notice that the corresponding sentence has been
changed now, indicating the range, not the average).

> Line 222-223: Does the fine-grained binning help to determine better
> center of mass, or what is otherwise its advantage?

To avoid 'rho-like' effects also in the acceptance determination due to the
different MC/data spectra. Since we have large MC samples, this can be done,
while in data tag-and-probe statistics is limited and we have to correct
the large binning effect.

> Page 11, Figure 3: It would be nice to elaborate on the figure in the text
> more.

A comment will be added on the "maximum" of the cross-section at low pT.

> Line 256, 274: The "outlier component" is mentioned twice, but
> it is not clear where it enters in the formulae.

It has been changed to be clearer.

> Page 13, Figure 4: Bottom plots appear to be done in a hurry. The binning
> is unclear and the relation to the top plots is unclear (eg. some points
> present in the bottom plots appear to be missing in the top ones).

This is fixed now.

> Page 13, Table 6: Is the factor of 7 change of the B-fraction for the
> different pT bins an expected effect?

Yes, because the non-prompt J/psi receive a larger trasverse boost from the
decaying b-parent, therefore their pT spectrum is shifted to the right. 
See for example the corresponding result at CDF [hep-ex/0412021, figure 13]

> 
> Conclusions:
> 
> We think the conclusions of the paper should focus on the data, point 
> out their relevance, and discuss the statistical and systematic
> uncertainties. 

OK.

> Nevertheless, in the comparison of the cross sections
> with the different theoretical predictions, one could make clearer
> statements instead of vaguely pointing out a general agreement.

This must be done with care, but we will change it in the draft. 

Most of your editorial comments have been taken into account, as you can
verify in the next paper draft.

