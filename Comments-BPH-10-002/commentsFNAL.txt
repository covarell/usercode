Dear FNAL people,

we went through the very detailed list of your comments.

For simplicity in the following we answer only to physics and major style 
comments. Most of minor style and grammar comments have been taken into 
account and will be implemented in the next paper version. 

> a) Polarization: handling is confusing. More should be said in the introduction and where
> appropriate in the text about lack of theoretical understanding of the Tevatron data and that
> variation in these models would produce major differences in the results if these were included in
> the simulation.

This will be changed (see answers to Andreas).
 
> b) A lot of the systematic errors as given in the tables fluctuate a lot. Have these been checked
> carefully or just came out of the program? Is this just due to statistical fluctuation in the
> procedure to determine the assigned systematic uncertainty? Should large bin size be used to
> avoid these fluctuations and get a more robust evaluation of the uncertainty?

General concern. It is being discussed between us, the conveners, the ARC and
the Phys-Coord. The final choice will be posted in HN.

> c) Acceptance correction: J/psi generated in bin n which was then found in bin n+1 (or vice
> versa). Has resolution effects taken into account when corrected for the acceptance?

Yes (see lines 176-181 and 233-236). 

> d) Global vs Tracker muon: Does the MC gives the same ratio of Global vs Tracker muon as the
> data? 

Within a few %.

> Is there a difference between the acceptance for the Global and Tracker muon?

Since the acceptance is defined as the region where the detection is possible
at all (> 10%), irrespective of the category, this difference is negligible. 
The two have a very different efficiency, but this is taken from data (so
with the correct proportion of Global and Tracker muons.
 
> e) Luminosity is the single biggest source of systematic uncertainty. There should be some
> discussion about this.
> 
> f) The systematics are significantly smaller for lambda=-1. Which one of the individual
> contributions is affected so much by the polarization? Please mention this in the text.

No, they are not, if you consider the fractional uncertainty.
 
> g) Question on the efficiency calculation: With two muons in the same event, it's easier to fulfill
> the trigger requirement. According to eqn 4, the trigger efficiency is calculated as the product of
> individual efficiency from the two muons which is not correct.

The trigger is a double-muon trigger, so what we say is correct (at least in
principle). Detector-related effects are corrected by the rho factor. 
 
> h) Selecting the right J/psi: on average, there are 1.07 J/psi candidates per selected event. There
> is a prescription of how to pick the correct J/psi. From Monte Carlo studies, how often this
> arbitration (i.e. largest pT) picks the correct candidate?

What really matters is the purity. The pT comparison is done just for a very 
small fraction of events (<< 7%) where one has two combinations *with the
same purity* (e.g. two Global-Global muon pairs). The purity choice picks up
the correct candidate in almost 100% of the cases in prompt signal MC.
 
> i) Resolution for prompt and non-prompt J/Psi: Both samples are assumed to have the same
> resolution model in decay lifetime. It is possible that they are different. It may be more
> appropriate to study the systematic is to use different functional form for the two.

If we add different parameters for prompt and non-prompt resolution the fit
does not converge at all. The correct thing to do to take into account 
additional dependencies (e.g. number of tracks in the event) is to use the
per-event ct error, which will be done in the next paper.
 
> j) Comparison of the prompt J/psi data with model: A statement should be made that none of the
> model describe the prompt data.

Being discussed.
 
> k) The tables shows results of Pt below 4.5 GeV/c for the rapidity range above 1.2. In fact, for
> the rapidity range between 1.6 to 2.4, the Pt coverage goes all the way to 0 GeV/c. Also this is
> shown in Fig. 3. However, the cross-section results report only covered the Pt range above 4.5
> GeV/c. There was no explanation in the text about this. Looking at figure 5, one can see clear
> differences between the various models in the low Pt region. So the data is there but its
> discrimination power is not being used. Please explain.

Using the full acceptance, the total value must be defined in a complicated
pT-|y| region that is harder to compare with theory (see comments to Giovanni). 
 
> l) Eqn. 4, line#192 there is a correction \rho to the efficiency that takes into account the effects of
> bin size and factorization, which is determined from MC to be 0.095 with a (sic) RMS of 0.11
> Why is this then not set to 0, with a comment that this correction is negligible?
> A couple of paragraphs later, line #205 they are talking about a non-vanishing value for \rho the
> variation of which is then is used as an estimate for the systematic error. How does this relates to
> the RMS mentioned above?

See answers to most reviewers. All this part will be removed as the RMS is
ill-defined.
 
> m) primary vertex estimation, line 306: why would the difference of two methods be a good
> estimate for the systematics here? Is this a big effect? There is no indication how important this
> is, 

Yes, it is in table 6, 4th column.

> and if it is a sizable effect, just using a different method and take this as a systematic error is
> not a very scientific approach.

We agree on that. The point is that, in start-up conditions, different factors 
enter the beam-spot and primary vertex estimation. The beam spot
gives a worst estimation because of averaging many events. On the other hand,
the primary vertex can suffer from other uncertainties as misassignment due
to pile-up, bias due to the displacement of tracks in bbbar events, etc.
Those were not studied in detail due to the limited amount of statistics:
so we're not able at this stage to tell which of the two methods gives the
worse resolution (resolution parameter results of the two fits are actually
compatible within errors) and we reflect our ignorance in taking the
full difference of the two. 

> n) In the Introduction
> "they generally fail to describe simultaneously the cross-section and polarization". My
> understanding is that NNLO plus other standard QCD contributions can now account for both the
> cross section and the polarization; would suggest that the authors consult with the people listed
> in the Acknowledgments.

??? 

> The paper would emphasize that J/psi production has TWO main divisions: prompt and nonprompt.
> Prompt production includes feed-downs from the psi(2S) and chi states, but these have
> extremely short lifetimes and do not lead to secondary vertices. In the color octet and
> evaporation models there is a similar feed-down mechanism.

?????

> Figure 1: The font is really tiny, especially the subscript in the legend defining rapidity range.
> Also, is the dotted line the exponential background fit? Or is it the same-sign background? If so,
> was there normalization applied? Or did it just come out right?

For the fonts, see answers to Giovanni. The dotted line is the exponential
background fit.
 
> Line 198: You mention that the signal purity is 95%. Should this be mentioned earlier? You use
> this +/- 100 MeV window before.	

No, 95% is just the purity for the Global-Global category, that is not 
separated in the rest in the analysis. We can use it there since the vertexing
efficiency depends only on tracking (so a muon being Global or Tracker is
irrelevant).
 
> Figure 4: the bottom pull plots are ugly due to the large numbers of points. Iâm not sure what to
> do about that. Rebin the central bins? Leave it full? Put in some representative points? (I don't
> like that one.) Maybe go for a log-like scale (meaning keep the sign for the negative sign, but
> make the negative a -log(|negative value|)?

We tried to do less bins (the fit is unbinned, so the result does not change),
but in this way the upper figures become ugly...

> Figure 5 & 6: The data is plotted at the center of the bin, although the bin centroid is to the left of
> that. There is a famous paper by Terry Wyatt on how to do this right.

No, they are not. We use average and RMS pT in the bin from table 5.

> Figure 5 & 6: the CASCADE wiggles are unphysical and ugly. Shouldnât you run more MC, or
> smooth or something?

General concern, an answer will be posted in HN. 
 
> Line 12: Just a comment -- It would be great if we could advertise better results at high Pt but I
> am not sure that is really true. Our highest bin extends from 10-30 but with a mean Pt of only 13.
> We might want to consider whether we have more data to add at some future date for a
> polarization study that I hope we can do.

Yes. At some future date.
 
> Line 94-95: I am very surprised that such a large region is used. Was that necessary?

Yes. For the z extension, this is the normal PV acceptance region.
In x-y this is enlarged w.r.t. other analyses because we want to effectively
keep all the B events (remember that a J/psi with a ctau of 2 mm, perfectly
possible in case of a B-decay, and a pT of 20 GeV will decay
2*20/3.1 = 13 mm away from the PV).

> Line 99-100: If I understand this correctly, the worst case correction is about 2x10^-3 over the
> whole range of Pt and eta under consideration. It might be worth saying something like that. It
> seems that this source of uncertainty is claimed to be negligible on line 236.

Correct.

> Figure 1: It seems to be a CMS practice to not put the fitted number of signals events on
> histograms. Is there a reason for that?

Same comments as above. If we want to add even more stuff we have to remove
some.
 
> Also related to Figure 1  is there some reason for using these coarse eta bins? Later, it is
> explained that the data were binned more finely but were added up into these coarse bins. Are
> the data available in the finer bins? One reason for mentioning this is that here were some etadependent
> issues in the trigger and one wonders if they were all removed or shown to be
> unimportant

No real reason, but we think three figures are more compact (and give a better
idea of thevarying resolution) than 33 figures (i.e. one per bin).

> Line 220-22: this means that cross sections are available (corrected??) over the whole grid of
> Fig. 2. Are these truly available as fully corrected quantities? Expect people to ask for them.

No, they aren't. Signal yields are only calculated in the binning of Table 2.